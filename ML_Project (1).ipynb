{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "model.save('final_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXTdrsATInFl",
        "outputId": "ce7d3f19-00d2-47f0-8e91-2183d28537b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('final_model.keras')\n",
        "loaded_model = keras.models.load_model('final_model.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ9mNRveI5SG",
        "outputId": "927cfec4-0708-440a-8331-98415dc456a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check the current working directory\n",
        "print(\"Current Directory:\", os.getcwd())\n",
        "\n",
        "# List the files in the current directory\n",
        "print(\"Files in Directory:\", os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOJUvHLzJ75B",
        "outputId": "4529d6e2-bf81-4859-9082-3766c26dbfdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory: /content\n",
            "Files in Directory: ['.config', 'final_model.keras', 'drive', 'final_model.h5', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXCvagrGKGn5",
        "outputId": "833d6098-bc0d-4492-dfcb-9b0d2713f123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV files\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/ML_Project/malicious_dataset_2.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/ML_Project/safe_urls.csv')\n",
        "\n",
        "# Display the first few rows of each DataFrame to confirm they loaded correctly\n",
        "print(\"DataFrame 1 (Malicious Dataset):\")\n",
        "print(df1.head())\n",
        "\n",
        "print(\"\\nDataFrame 2 (Safe URLs):\")\n",
        "print(df2.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUMw8tFkKNs7",
        "outputId": "880c9fb4-0466-468e-8767-d8a814e496ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame 1 (Malicious Dataset):\n",
            "  http://cnfrmappsecurhomebusiness.github.io/\n",
            "0    http://f-protection-services.vercel.app/\n",
            "1           https://currently4326.weebly.com/\n",
            "2    https://sites.google.com/view/zxacq/home\n",
            "3                        https://urlz.fr/sALl\n",
            "4         https://cnnxmnwsieudjff.weebly.com/\n",
            "\n",
            "DataFrame 2 (Safe URLs):\n",
            "   1     google.com\n",
            "0  2    youtube.com\n",
            "1  3   facebook.com\n",
            "2  4      baidu.com\n",
            "3  5  wikipedia.org\n",
            "4  6      yahoo.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a label column to each DataFrame\n",
        "df1['label'] = 0  # Malicious links\n",
        "df2['label'] = 1  # Safe links\n",
        "\n",
        "# Combine the DataFrames\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Display the first few rows of the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be9WhZWmLGdc",
        "outputId": "11894789-c3e2-4e4b-b876-815345a1bb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame:\n",
            "  http://cnfrmappsecurhomebusiness.github.io/  label   1 google.com\n",
            "0    http://f-protection-services.vercel.app/      0 NaN        NaN\n",
            "1           https://currently4326.weebly.com/      0 NaN        NaN\n",
            "2    https://sites.google.com/view/zxacq/home      0 NaN        NaN\n",
            "3                        https://urlz.fr/sALl      0 NaN        NaN\n",
            "4         https://cnnxmnwsieudjff.weebly.com/      0 NaN        NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/ML_Project/malicious_dataset_2.csv', header=None)\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/ML_Project/safe_urls.csv', header=0)\n",
        "\n",
        "# Assign proper column names\n",
        "df1.columns = ['url']  # Malicious URLs\n",
        "df2.columns = ['url']   # Safe URLs\n",
        "\n",
        "# Add labels to each DataFrame\n",
        "df1['label'] = 0  # Malicious links\n",
        "df2['label'] = 1  # Safe links\n",
        "\n",
        "# Combine the DataFrames\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Display the first few rows of the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "fBwt22YbL5lj",
        "outputId": "5753f24a-413d-46a6-edf3-da00570e1d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 2 elements, new values have 1 elements",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-0b9e98930f74>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assign proper column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Malicious URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# Safe URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Add labels to each DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6312\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mproperties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 2 elements, new values have 1 elements"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the safe URLs CSV and check its structure\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/ML_Project/safe_urls.csv', header=None)  # Load without specifying headers\n",
        "print(\"Safe URLs DataFrame:\")\n",
        "print(df2.head())\n",
        "print(f\"Shape of df2: {df2.shape}\")  # Print shape to understand the number of columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8uuuhGeME2-",
        "outputId": "774e7b4e-cd98-4628-cc99-9da00190166c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Safe URLs DataFrame:\n",
            "   0              1\n",
            "0  1     google.com\n",
            "1  2    youtube.com\n",
            "2  3   facebook.com\n",
            "3  4      baidu.com\n",
            "4  5  wikipedia.org\n",
            "Shape of df2: (1000000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from urllib.parse import urlparse\n",
        "from google.colab import drive\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Mount Google Drive to load the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset directly from Google Drive\n",
        "data_path = '/content/drive/MyDrive/ML_Project/combined_url_dataset.xlsx'  # Update path\n",
        "combined_df = pd.read_excel(data_path)\n",
        "\n",
        "# Use a smaller sample (10%) of the data for testing\n",
        "combined_df = combined_df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Feature Extraction for Additional Features\n",
        "def calculate_entropy(url):\n",
        "    \"\"\"Calculate Shannon entropy of characters in the URL.\"\"\"\n",
        "    counts = Counter(url)\n",
        "    total_chars = len(url)\n",
        "    return -sum((count / total_chars) * np.log2(count / total_chars) for count in counts.values())\n",
        "\n",
        "def subdomain_count(url):\n",
        "    \"\"\"Count the number of subdomains in the URL.\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.netloc.count('.')\n",
        "\n",
        "# Apply feature extraction\n",
        "combined_df['url_length'] = combined_df['url'].apply(len)\n",
        "combined_df['entropy'] = combined_df['url'].apply(calculate_entropy)\n",
        "combined_df['subdomain_count'] = combined_df['url'].apply(subdomain_count)\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer and apply dimensionality reduction\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(combined_df['url'])\n",
        "\n",
        "# Use TruncatedSVD for dimensionality reduction\n",
        "svd = TruncatedSVD(n_components=200)\n",
        "X_tfidf_reduced = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# Scale and combine with additional features\n",
        "X_additional = combined_df[['url_length', 'entropy', 'subdomain_count']].values\n",
        "scaler = StandardScaler()\n",
        "X_additional_scaled = scaler.fit_transform(X_additional)\n",
        "\n",
        "# Combine TF-IDF reduced features with scaled additional features\n",
        "X_combined = np.hstack((X_tfidf_reduced, X_additional_scaled))\n",
        "\n",
        "# Define the target variable\n",
        "y = combined_df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the ANN Model with a smaller structure\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(16, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with EarlyStopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DUzov48-TR0",
        "outputId": "1f1e34b1-1e49-4cbe-8fd8-baa29a3e5687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.2260 - val_accuracy: 1.0000 - val_loss: 7.2583e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.7372e-04 - val_accuracy: 1.0000 - val_loss: 1.3503e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.0277e-04 - val_accuracy: 1.0000 - val_loss: 4.7335e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.8517e-05 - val_accuracy: 1.0000 - val_loss: 2.0445e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6348e-05 - val_accuracy: 1.0000 - val_loss: 9.7474e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 8.2633e-06 - val_accuracy: 1.0000 - val_loss: 4.9607e-06\n",
            "Epoch 7/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.2241e-06 - val_accuracy: 1.0000 - val_loss: 2.5862e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.0812e-06 - val_accuracy: 1.0000 - val_loss: 1.3609e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.2560e-06 - val_accuracy: 1.0000 - val_loss: 7.4039e-07\n",
            "Epoch 10/10\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.8240e-07 - val_accuracy: 1.0000 - val_loss: 4.0634e-07\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.5570e-07\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG0hckMBt8rm",
        "outputId": "5df7931f-20ec-486f-83c0-2203206f4dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/ML_Project/malicious_dataset_3.csv'\n"
      ],
      "metadata": {
        "id": "0fCWLRyDvBia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data_path = '/content/drive/MyDrive/ML_Project/combined_url_dataset2.xlsx'\n",
        "df = pd.read_excel(data_path)\n",
        "\n",
        "# Shuffle the data\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the shuffled data back to a new Excel file\n",
        "df.to_excel('/content/drive/MyDrive/ML_Project/shuffled_combined_url_dataset2.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "n3l-_Ubhs4TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "from scipy.stats import entropy\n",
        "from urllib.parse import urlparse\n",
        "from google.colab import drive\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Mount Google Drive to load the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset directly from Google Drive\n",
        "data_path = '/content/drive/MyDrive/ML_Project/combined_url_dataset2.xlsx'  # Update path\n",
        "combined_df = pd.read_excel(data_path)\n",
        "\n",
        "# Use a smaller sample (10%) of the data for testing\n",
        "combined_df = combined_df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Feature Extraction for Additional Features\n",
        "def calculate_entropy(url):\n",
        "    \"\"\"Calculate Shannon entropy of characters in the URL.\"\"\"\n",
        "    counts = Counter(url)\n",
        "    total_chars = len(url)\n",
        "    return -sum((count / total_chars) * np.log2(count / total_chars) for count in counts.values())\n",
        "\n",
        "def subdomain_count(url):\n",
        "    \"\"\"Count the number of subdomains in the URL.\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.netloc.count('.')\n",
        "\n",
        "# Apply feature extraction\n",
        "combined_df['url_length'] = combined_df['url'].apply(len)\n",
        "combined_df['entropy'] = combined_df['url'].apply(calculate_entropy)\n",
        "combined_df['subdomain_count'] = combined_df['url'].apply(subdomain_count)\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer and apply dimensionality reduction\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(combined_df['url'])\n",
        "\n",
        "# Use TruncatedSVD for dimensionality reduction\n",
        "svd = TruncatedSVD(n_components=200)\n",
        "X_tfidf_reduced = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# Scale and combine with additional features\n",
        "X_additional = combined_df[['url_length', 'entropy', 'subdomain_count']].values\n",
        "scaler = StandardScaler()\n",
        "X_additional_scaled = scaler.fit_transform(X_additional)\n",
        "\n",
        "# Combine TF-IDF reduced features with scaled additional features\n",
        "X_combined = np.hstack((X_tfidf_reduced, X_additional_scaled))\n",
        "\n",
        "# Define the target variable\n",
        "y = combined_df['label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the ANN Model with a smaller structure\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(16, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with EarlyStopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJadcBHP1lez",
        "outputId": "8e1d897b-bde0-48d6-a141-502a4546f115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8846 - loss: 0.3502 - val_accuracy: 0.9403 - val_loss: 0.1502\n",
            "Epoch 2/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9423 - loss: 0.1485 - val_accuracy: 0.9428 - val_loss: 0.1434\n",
            "Epoch 3/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9450 - loss: 0.1435 - val_accuracy: 0.9428 - val_loss: 0.1424\n",
            "Epoch 4/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9453 - loss: 0.1428 - val_accuracy: 0.9433 - val_loss: 0.1418\n",
            "Epoch 5/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9440 - loss: 0.1445 - val_accuracy: 0.9440 - val_loss: 0.1429\n",
            "Epoch 6/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9450 - loss: 0.1425 - val_accuracy: 0.9437 - val_loss: 0.1412\n",
            "Epoch 7/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9439 - loss: 0.1440 - val_accuracy: 0.9436 - val_loss: 0.1435\n",
            "Epoch 8/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9451 - loss: 0.1412 - val_accuracy: 0.9442 - val_loss: 0.1417\n",
            "Epoch 9/50\n",
            "\u001b[1m1049/1049\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9448 - loss: 0.1401 - val_accuracy: 0.9423 - val_loss: 0.1443\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9453 - loss: 0.1450\n",
            "Test Accuracy: 0.9435\n"
          ]
        }
      ]
    }
  ]
}